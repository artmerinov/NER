{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "from utils import set_random_seed, Config, load_ner_config\n",
    "from dataset import io2df, io2bio, padding, NERDataset, make_char_vocab, make_word_vocab\n",
    "from model import CNN_BiLSTM_CRF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_random_seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TR_PATH': 'data/supervised/train.txt',\n",
       " 'VA_PATH': 'data/supervised/dev.txt',\n",
       " 'TE_PATH': 'data/supervised/test.txt',\n",
       " 'MAX_SEQ_LEN': 64,\n",
       " 'MAX_WORD_LEN': 16,\n",
       " 'BATCH_SIZE': 128,\n",
       " 'LR': 0.001,\n",
       " 'REG_LAMBDA': 0.0001,\n",
       " 'MAX_GRAD_NORM': 100,\n",
       " 'NUM_EPOCHS': 10,\n",
       " 'WORD_EMBED_SIZE': 128,\n",
       " 'CHAR_EMBED_SIZE': 128,\n",
       " 'KERNEL_SIZE': 3,\n",
       " 'LSTM_HIDDEN_SIZE': 128,\n",
       " 'DROPOUT': 0.5}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config('config.yaml')\n",
    "config.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles = io2df(config.TR_PATH)\n",
    "va_titles = io2df(config.VA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags_fine_grained</th>\n",
       "      <th>tags_coarse_grained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Paul, International, airport, .]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[It, starred, Hicks, 's, wife, ,, Ellaline, Te...</td>\n",
       "      <td>[O, O, person-artist/author, O, O, O, person-a...</td>\n",
       "      <td>[O, O, person, O, O, O, person, person, O, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[``, Time, ``, magazine, said, the, film, was,...</td>\n",
       "      <td>[O, art-writtenart, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[O, art, O, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Pakistani, scientists, and, engineers, ', wor...</td>\n",
       "      <td>[O, O, O, O, O, O, O, organization-other, O, O...</td>\n",
       "      <td>[O, O, O, O, O, O, O, organization, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[In, February, 2008, ,, Church, 's, Chicken, e...</td>\n",
       "      <td>[O, O, O, O, organization-company, organizatio...</td>\n",
       "      <td>[O, O, O, O, organization, organization, organ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131762</th>\n",
       "      <td>131762</td>\n",
       "      <td>[In, response, ,, the, states, who, had, ratif...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, other-law, O, O, o...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, other, O, O, organ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131763</th>\n",
       "      <td>131763</td>\n",
       "      <td>[They, have, long, been, used, as, containers,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131764</th>\n",
       "      <td>131764</td>\n",
       "      <td>[In, 1911, he, came, into, possession, of, the...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131765</th>\n",
       "      <td>131765</td>\n",
       "      <td>[The, Lutici, tribes, in, 983, formed, the, Li...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131766</th>\n",
       "      <td>131766</td>\n",
       "      <td>[Afterward, ,, he, was, arrested, as, a, Russi...</td>\n",
       "      <td>[O, O, O, O, O, O, O, location-GPE, O, O, loca...</td>\n",
       "      <td>[O, O, O, O, O, O, O, location, O, O, location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131767 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              words  \\\n",
       "0            0                  [Paul, International, airport, .]   \n",
       "1            1  [It, starred, Hicks, 's, wife, ,, Ellaline, Te...   \n",
       "2            2  [``, Time, ``, magazine, said, the, film, was,...   \n",
       "3            3  [Pakistani, scientists, and, engineers, ', wor...   \n",
       "4            4  [In, February, 2008, ,, Church, 's, Chicken, e...   \n",
       "...        ...                                                ...   \n",
       "131762  131762  [In, response, ,, the, states, who, had, ratif...   \n",
       "131763  131763  [They, have, long, been, used, as, containers,...   \n",
       "131764  131764  [In, 1911, he, came, into, possession, of, the...   \n",
       "131765  131765  [The, Lutici, tribes, in, 983, formed, the, Li...   \n",
       "131766  131766  [Afterward, ,, he, was, arrested, as, a, Russi...   \n",
       "\n",
       "                                        tags_fine_grained  \\\n",
       "0                                            [O, O, O, O]   \n",
       "1       [O, O, person-artist/author, O, O, O, person-a...   \n",
       "2       [O, art-writtenart, O, O, O, O, O, O, O, O, O,...   \n",
       "3       [O, O, O, O, O, O, O, organization-other, O, O...   \n",
       "4       [O, O, O, O, organization-company, organizatio...   \n",
       "...                                                   ...   \n",
       "131762  [O, O, O, O, O, O, O, O, O, other-law, O, O, o...   \n",
       "131763  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "131764   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "131765  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "131766  [O, O, O, O, O, O, O, location-GPE, O, O, loca...   \n",
       "\n",
       "                                      tags_coarse_grained  \n",
       "0                                            [O, O, O, O]  \n",
       "1       [O, O, person, O, O, O, person, person, O, per...  \n",
       "2       [O, art, O, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "3       [O, O, O, O, O, O, O, organization, O, O, O, O...  \n",
       "4       [O, O, O, O, organization, organization, organ...  \n",
       "...                                                   ...  \n",
       "131762  [O, O, O, O, O, O, O, O, O, other, O, O, organ...  \n",
       "131763  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "131764   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "131765  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "131766  [O, O, O, O, O, O, O, location, O, O, location...  \n",
       "\n",
       "[131767 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag -> tagID\n",
    "TAG2IDX = load_ner_config('ner_tags/ner_fine_grained.json')\n",
    "IDX2TAG = {i: t for t, i in TAG2IDX.items()}\n",
    "\n",
    "# word -> wordID\n",
    "WORD2IDX = make_word_vocab(filepath=config.TR_PATH, support=5)\n",
    "IDX2WORD = {i: w for w, i in WORD2IDX.items()}\n",
    "with open('tokenizers/word2idx.json', 'w') as f:\n",
    "    json.dump(WORD2IDX, f, indent=4)\n",
    "\n",
    "# char -> charID\n",
    "CHAR2IDX = make_char_vocab(filepath=config.TR_PATH, support=100)\n",
    "with open('tokenizers/char2idx.json', 'w') as f:\n",
    "    json.dump(CHAR2IDX, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply TAG2IDX\n",
    "tr_titles['tag_ids'] = tr_titles['tags_fine_grained'].transform(lambda x: [TAG2IDX[tag] for tag in x])\n",
    "va_titles['tag_ids'] = va_titles['tags_fine_grained'].transform(lambda x: [TAG2IDX[tag] for tag in x])\n",
    "\n",
    "# apply WORD2IDX\n",
    "tr_titles['word_ids'] = tr_titles['words'].transform(lambda x: [WORD2IDX[w] if w in WORD2IDX else WORD2IDX['UKN'] for w in x])\n",
    "va_titles['word_ids'] = va_titles['words'].transform(lambda x: [WORD2IDX[w] if w in WORD2IDX else WORD2IDX['UKN'] for w in x])\n",
    "\n",
    "# apply padding\n",
    "tr_titles['word_ids'] = tr_titles['word_ids'].transform(padding, max_len=config.MAX_SEQ_LEN)\n",
    "tr_titles['tag_ids'] = tr_titles['tag_ids'].transform(padding, max_len=config.MAX_SEQ_LEN)\n",
    "\n",
    "va_titles['word_ids'] = va_titles['word_ids'].transform(padding, max_len=config.MAX_SEQ_LEN)\n",
    "va_titles['tag_ids'] = va_titles['tag_ids'].transform(padding, max_len=config.MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags_fine_grained</th>\n",
       "      <th>tags_coarse_grained</th>\n",
       "      <th>tag_ids</th>\n",
       "      <th>word_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Paul, International, airport, .]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[586, 170, 711, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[It, starred, Hicks, 's, wife, ,, Ellaline, Te...</td>\n",
       "      <td>[O, O, person-artist/author, O, O, O, person-a...</td>\n",
       "      <td>[O, O, person, O, O, O, person, person, O, per...</td>\n",
       "      <td>[1, 1, 52, 1, 1, 1, 51, 51, 1, 51, 51, 1, 0, 0...</td>\n",
       "      <td>[35, 1601, 15202, 22, 659, 3, 1, 1, 6, 5586, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[``, Time, ``, magazine, said, the, film, was,...</td>\n",
       "      <td>[O, art-writtenart, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[O, art, O, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[10, 2065, 10, 1045, 381, 2, 76, 11, 10, 9, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Pakistani, scientists, and, engineers, ', wor...</td>\n",
       "      <td>[O, O, O, O, O, O, O, organization-other, O, O...</td>\n",
       "      <td>[O, O, O, O, O, O, O, organization, O, O, O, O...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 33, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[5323, 5587, 6, 6537, 59, 603, 21, 1, 78, 1076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[In, February, 2008, ,, Church, 's, Chicken, e...</td>\n",
       "      <td>[O, O, O, O, organization-company, organizatio...</td>\n",
       "      <td>[O, O, O, O, organization, organization, organ...</td>\n",
       "      <td>[1, 1, 1, 1, 29, 29, 29, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[25, 187, 139, 3, 340, 22, 6282, 922, 2, 541, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131762</th>\n",
       "      <td>131762</td>\n",
       "      <td>[In, response, ,, the, states, who, had, ratif...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, other-law, O, O, o...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, other, O, O, organ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 48, 1, 1, 33, 33, ...</td>\n",
       "      <td>[25, 1235, 3, 2, 1110, 49, 40, 8312, 2, 14192,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131763</th>\n",
       "      <td>131763</td>\n",
       "      <td>[They, have, long, been, used, as, containers,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[198, 53, 470, 47, 68, 17, 19121, 16, 8367, 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131764</th>\n",
       "      <td>131764</td>\n",
       "      <td>[In, 1911, he, came, into, possession, of, the...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[25, 2509, 29, 383, 57, 5299, 5, 2, 1, 28249, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131765</th>\n",
       "      <td>131765</td>\n",
       "      <td>[The, Lutici, tribes, in, 983, formed, the, Li...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12, 1, 3938, 7, 1, 391, 2, 1, 17656, 3, 5239,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131766</th>\n",
       "      <td>131766</td>\n",
       "      <td>[Afterward, ,, he, was, arrested, as, a, Russi...</td>\n",
       "      <td>[O, O, O, O, O, O, O, location-GPE, O, O, loca...</td>\n",
       "      <td>[O, O, O, O, O, O, O, location, O, O, location...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 22, 1, 1, 22, 1, 1, 1, 1...</td>\n",
       "      <td>[24359, 3, 29, 11, 2684, 17, 9, 604, 7094, 19,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131767 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              words  \\\n",
       "0            0                  [Paul, International, airport, .]   \n",
       "1            1  [It, starred, Hicks, 's, wife, ,, Ellaline, Te...   \n",
       "2            2  [``, Time, ``, magazine, said, the, film, was,...   \n",
       "3            3  [Pakistani, scientists, and, engineers, ', wor...   \n",
       "4            4  [In, February, 2008, ,, Church, 's, Chicken, e...   \n",
       "...        ...                                                ...   \n",
       "131762  131762  [In, response, ,, the, states, who, had, ratif...   \n",
       "131763  131763  [They, have, long, been, used, as, containers,...   \n",
       "131764  131764  [In, 1911, he, came, into, possession, of, the...   \n",
       "131765  131765  [The, Lutici, tribes, in, 983, formed, the, Li...   \n",
       "131766  131766  [Afterward, ,, he, was, arrested, as, a, Russi...   \n",
       "\n",
       "                                        tags_fine_grained  \\\n",
       "0                                            [O, O, O, O]   \n",
       "1       [O, O, person-artist/author, O, O, O, person-a...   \n",
       "2       [O, art-writtenart, O, O, O, O, O, O, O, O, O,...   \n",
       "3       [O, O, O, O, O, O, O, organization-other, O, O...   \n",
       "4       [O, O, O, O, organization-company, organizatio...   \n",
       "...                                                   ...   \n",
       "131762  [O, O, O, O, O, O, O, O, O, other-law, O, O, o...   \n",
       "131763  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "131764   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "131765  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "131766  [O, O, O, O, O, O, O, location-GPE, O, O, loca...   \n",
       "\n",
       "                                      tags_coarse_grained  \\\n",
       "0                                            [O, O, O, O]   \n",
       "1       [O, O, person, O, O, O, person, person, O, per...   \n",
       "2       [O, art, O, O, O, O, O, O, O, O, O, O, O, O, O...   \n",
       "3       [O, O, O, O, O, O, O, organization, O, O, O, O...   \n",
       "4       [O, O, O, O, organization, organization, organ...   \n",
       "...                                                   ...   \n",
       "131762  [O, O, O, O, O, O, O, O, O, other, O, O, organ...   \n",
       "131763  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "131764   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "131765  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "131766  [O, O, O, O, O, O, O, location, O, O, location...   \n",
       "\n",
       "                                                  tag_ids  \\\n",
       "0       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       [1, 1, 52, 1, 1, 1, 51, 51, 1, 51, 51, 1, 0, 0...   \n",
       "2       [1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 33, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "4       [1, 1, 1, 1, 29, 29, 29, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "131762  [1, 1, 1, 1, 1, 1, 1, 1, 1, 48, 1, 1, 33, 33, ...   \n",
       "131763  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "131764  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "131765  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "131766  [1, 1, 1, 1, 1, 1, 1, 22, 1, 1, 22, 1, 1, 1, 1...   \n",
       "\n",
       "                                                 word_ids  \n",
       "0       [586, 170, 711, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       [35, 1601, 15202, 22, 659, 3, 1, 1, 6, 5586, 8...  \n",
       "2       [10, 2065, 10, 1045, 381, 2, 76, 11, 10, 9, 1,...  \n",
       "3       [5323, 5587, 6, 6537, 59, 603, 21, 1, 78, 1076...  \n",
       "4       [25, 187, 139, 3, 340, 22, 6282, 922, 2, 541, ...  \n",
       "...                                                   ...  \n",
       "131762  [25, 1235, 3, 2, 1110, 49, 40, 8312, 2, 14192,...  \n",
       "131763  [198, 53, 470, 47, 68, 17, 19121, 16, 8367, 38...  \n",
       "131764  [25, 2509, 29, 383, 57, 5299, 5, 2, 1, 28249, ...  \n",
       "131765  [12, 1, 3938, 7, 1, 391, 2, 1, 17656, 3, 5239,...  \n",
       "131766  [24359, 3, 29, 11, 2684, 17, 9, 604, 7094, 19,...  \n",
       "\n",
       "[131767 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131767/131767 [00:38<00:00, 3422.56it/s]\n",
      "100%|██████████| 18824/18824 [00:04<00:00, 4143.52it/s]\n"
     ]
    }
   ],
   "source": [
    "tr_dataset = NERDataset(data=tr_titles, idx2word=IDX2WORD, char2idx=CHAR2IDX, max_word_len=config.MAX_WORD_LEN)\n",
    "va_dataset = NERDataset(data=va_titles, idx2word=IDX2WORD, char2idx=CHAR2IDX, max_word_len=config.MAX_WORD_LEN)\n",
    "\n",
    "tr_dataloader = DataLoader(dataset=tr_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "va_dataloader = DataLoader(dataset=va_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_ids': tensor([586, 170, 711,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0]),\n",
       " 'char_ids': tensor([[38,  3, 14,  ...,  0,  0,  0],\n",
       "         [37,  5,  4,  ...,  0,  0,  0],\n",
       "         [ 3,  6,  8,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [38, 29, 43,  ...,  0,  0,  0],\n",
       "         [38, 29, 43,  ...,  0,  0,  0],\n",
       "         [38, 29, 43,  ...,  0,  0,  0]]),\n",
       " 'tag_ids': tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embedding.weight                    initialized w with Xavier            parameters #: 4442240\n",
      "char_embedding.char_embedding.weight     initialized w with Xavier            parameters #: 15488\n",
      "char_embedding.char_conv1d.weight        initialized w with Xavier            parameters #: 49152\n",
      "char_embedding.char_conv1d.bias          initialized b with zero              parameters #: 128\n",
      "lstm.weight_ih_l0                        initialized w with Xavier            parameters #: 65536\n",
      "lstm.weight_hh_l0                        initialized w with Xavier            parameters #: 16384\n",
      "lstm.weight_ih_l0_reverse                initialized w with Xavier            parameters #: 65536\n",
      "lstm.weight_hh_l0_reverse                initialized w with Xavier            parameters #: 16384\n",
      "fc.weight                                initialized w with Xavier            parameters #: 8704\n",
      "fc.bias                                  initialized b with zero              parameters #: 68\n",
      "crf.start_transitions                    initialized b with zero              parameters #: 68\n",
      "crf.end_transitions                      initialized b with zero              parameters #: 68\n",
      "crf.transitions                          initialized w with Xavier            parameters #: 4624\n",
      "CNN_BiLSTM_CRF(\n",
      "  (word_embedding): Embedding(34705, 128, padding_idx=0)\n",
      "  (char_embedding): CharCNN(\n",
      "    (char_embedding): Embedding(121, 128, padding_idx=0)\n",
      "    (char_conv1d): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(256, 64, bias=False, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=68, bias=True)\n",
      "  (crf): CRF(num_tags=68)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = CNN_BiLSTM_CRF(\n",
    "    word_embed_size  = config.WORD_EMBED_SIZE,\n",
    "    char_embed_size  = config.CHAR_EMBED_SIZE,\n",
    "    kernel_size      = config.KERNEL_SIZE,\n",
    "    lstm_hidden_size = config.LSTM_HIDDEN_SIZE,\n",
    "    dropout          = config.DROPOUT,\n",
    "    word_voc_size    = len(WORD2IDX),\n",
    "    char_voc_size    = len(CHAR2IDX),\n",
    "    tag_voc_size     = len(TAG2IDX),\n",
    ").to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = 'weights'\n",
    "if not os.path.exists(weights_folder):\n",
    "    os.makedirs(weights_folder)\n",
    "    \n",
    "runs_folder = 'runs'\n",
    "if not os.path.exists(runs_folder):\n",
    "    os.makedirs(runs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.LR)\n",
    "\n",
    "# Make tensorboard writer\n",
    "writer = tensorboard.SummaryWriter(log_dir='/runs')\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    print(f\"EPOCH: {epoch:02d}\")\n",
    "\n",
    "    # TRAINING PHASE\n",
    "    \n",
    "    tr_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for batch_num, tr_batch in enumerate(tr_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tr_xs = tr_batch['word_ids'].to(device) # [batch_size, max_seq_len]\n",
    "        tr_cs = tr_batch['char_ids'].to(device) # [batch_size, max_seq_len, max_word_len]\n",
    "        tr_ys = tr_batch['tag_ids'].to(device) # [batch_size, max_seq_len]\n",
    "        tr_mask = (tr_ys > 0).bool()\n",
    "        \n",
    "        # Calculate loss\n",
    "        tr_emission_scores = model(word_ids=tr_xs, char_ids=tr_cs).to(device) # [batch_size, max_seq_len, max_word_len]\n",
    "        tr_loss = model.loss_fn(emission_scores=tr_emission_scores, tags=tr_ys, mask=tr_mask)\n",
    "        tr_losses.append(tr_loss.item())\n",
    "        \n",
    "        # Calculate loss with regularizarion\n",
    "        reg_loss = model.regularization_loss_fn(lam=config.REG_LAMBDA)\n",
    "        total_loss = tr_loss + reg_loss\n",
    "        \n",
    "        if batch_num % 100 == 0:\n",
    "            print(f'batch_num={batch_num:04d}/{tr_dataloader.__len__()} | CRF LOSS: {tr_loss:.2f} | REG LOSS {reg_loss:.2f} | TOTAL LOSS {total_loss:.2f}')\n",
    "        \n",
    "        # Backward pass: compute gradient of the loss w.r.t. all learnable parameters\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Clip computed gradients\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=config.MAX_GRAD_NORM)\n",
    "        \n",
    "        # Optimize: update the weights using Adam optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "    # VALIDATION PHASE\n",
    "    \n",
    "    va_losses = []\n",
    "    \n",
    "    batch_preds = []\n",
    "    batch_trues = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for va_batch in tqdm(va_dataloader, total=va_dataloader.__len__()):\n",
    "            \n",
    "            va_xs = va_batch['word_ids'].to(device) # [batch_size, max_seq_len]\n",
    "            va_cs = va_batch['char_ids'].to(device) # [batch_size, max_seq_len, max_word_len]\n",
    "            va_ys = va_batch['tag_ids'].to(device) # [batch_size, max_seq_len]\n",
    "            va_mask = (va_ys > 0).bool()\n",
    "\n",
    "            # Forward pass: compute predicted output by passing input to the model\n",
    "            va_emission_scores = model(word_ids=va_xs, char_ids=va_cs).to(device) # [batch_size, max_seq_len]\n",
    "            va_preds = torch.tensor(model.decode(va_emission_scores)).to(device)\n",
    "            va_loss = model.loss_fn(emission_scores=va_emission_scores, tags=va_ys, mask=va_mask)\n",
    "            va_losses.append(va_loss.item())\n",
    "\n",
    "            for row_id, true in enumerate(va_ys):\n",
    "                # do not count padding\n",
    "                true_tags = true[va_mask[row_id]]\n",
    "                # idx2tag\n",
    "                true_tags = [IDX2TAG[idx] for idx in true_tags.tolist()]\n",
    "                # convert to the format expected by seqeval\n",
    "                true_tags = io2bio(true_tags)\n",
    "                batch_trues.append(true_tags)\n",
    "\n",
    "            for row_id, pred in enumerate(va_preds):\n",
    "                # do not count padding\n",
    "                pred_tags = pred[va_mask[row_id]]\n",
    "                # idx2tag\n",
    "                pred_tags = [IDX2TAG[idx] for idx in pred_tags.tolist()]\n",
    "                # convert to the format expected by seqeval\n",
    "                pred_tags = io2bio(pred_tags)\n",
    "                batch_preds.append(pred_tags)\n",
    "        \n",
    "#         for i in range(5):\n",
    "#             print('pred:', batch_preds[i])\n",
    "#             print('true:', batch_trues[i])\n",
    "#             print()\n",
    "\n",
    "    print(f\"TR AVG LOSS {np.mean(tr_losses)} | VA AVG LOSS {np.mean(va_losses)}\")\n",
    "    writer.add_scalar('tr/'+'loss', np.mean(tr_losses), global_step=epoch)\n",
    "    writer.add_scalar('va/'+'loss', np.mean(va_losses), global_step=epoch)\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram('tr/' + name + '_weight', param.data, global_step=epoch)\n",
    "        writer.add_histogram('tr/' + name + '_grad', param.grad, global_step=epoch)\n",
    "    \n",
    "    report = classification_report(y_true=batch_trues, y_pred=batch_preds, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"weights/model_epoch_{epoch:02d}.pt\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# v10 few-nerd-fine-graned-CNN-biLSTM-CRF\n",
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
