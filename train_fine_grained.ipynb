{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "from utils import set_random_seed, Config, load_ner_config\n",
    "from dataset import io2df, io2bio, padding, NERDataset\n",
    "from model import BiLSTM_CRF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_random_seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('config.yaml')\n",
    "config.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles = io2df(config.TR_PATH)\n",
    "va_titles = io2df(config.VA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag -> tagID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG2IDX = load_ner_config('ner_tags/ner_fine_grained.json')\n",
    "IDX2TAG = {i: t for t, i in TAG2IDX.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG2IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles['tags_ids'] = tr_titles['tags_fine_grained'].transform(lambda x: [TAG2IDX[tag] for tag in x])\n",
    "va_titles['tags_ids'] = va_titles['tags_fine_grained'].transform(lambda x: [TAG2IDX[tag] for tag in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token -> tokenID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_token_cntr(filepath):\n",
    "\n",
    "    token_cntr = Counter()\n",
    "    num_lines = sum(1 for _ in open(filepath, encoding=\"utf-8\"))\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, total=num_lines):\n",
    "            line = line.strip().split()\n",
    "            if line:\n",
    "                token, fine_tag = line\n",
    "                token_cntr[token] += 1\n",
    "    \n",
    "    return token_cntr\n",
    "\n",
    "token_cntr = calc_token_cntr(filepath=config.TR_PATH)\n",
    "\n",
    "\n",
    "MC = 50_000\n",
    "top_tokens = [token for token, _ in token_cntr.most_common(MC)]\n",
    "TOKEN2IDX = {token: i + 2 for i, token in enumerate(top_tokens)}\n",
    "TOKEN2IDX['PAD'] = 0\n",
    "TOKEN2IDX['UKN'] = 1\n",
    "\n",
    "with open('tokenizers/token2idx.json', 'w') as f:\n",
    "    json.dump(TOKEN2IDX, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles['tokens_ids'] = tr_titles['tokens'].transform(lambda x: [TOKEN2IDX[token] if token in TOKEN2IDX else TOKEN2IDX['UKN'] for token in x])\n",
    "va_titles['tokens_ids'] = va_titles['tokens'].transform(lambda x: [TOKEN2IDX[token] if token in TOKEN2IDX else TOKEN2IDX['UKN'] for token in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles['tokens_ids'] = tr_titles['tokens_ids'].transform(padding, max_len=config.SEQ_LEN)\n",
    "tr_titles['tags_ids'] = tr_titles['tags_ids'].transform(padding, max_len=config.SEQ_LEN)\n",
    "\n",
    "va_titles['tokens_ids'] = va_titles['tokens_ids'].transform(padding, max_len=config.SEQ_LEN)\n",
    "va_titles['tags_ids'] = va_titles['tags_ids'].transform(padding, max_len=config.SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = NERDataset(tr_titles)\n",
    "va_dataset = NERDataset(va_titles)\n",
    "\n",
    "tr_dataloader = DataLoader(dataset=tr_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "va_dataloader = DataLoader(dataset=va_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = tr_dataset.__getitem__(1)\n",
    "ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = BiLSTM_CRF(\n",
    "    embed_size     = config.EMBED_SIZE,\n",
    "    hidden_size    = config.HIDDEN_SIZE, \n",
    "    dropout        = config.DROPOUT,\n",
    "    token_voc_size = len(TOKEN2IDX), \n",
    "    tag_voc_size   = len(TAG2IDX),\n",
    ").to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = 'weights'\n",
    "if not os.path.exists(weights_folder):\n",
    "    os.makedirs(weights_folder)\n",
    "    \n",
    "runs_folder = '.runs'\n",
    "if not os.path.exists(runs_folder):\n",
    "    os.makedirs(runs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.LR)\n",
    "\n",
    "# Make tensorboard writer\n",
    "writer = tensorboard.SummaryWriter(log_dir='./runs')\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "\n",
    "    # TRAINING PHASE\n",
    "    \n",
    "    tr_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for tr_batch in tqdm(tr_dataloader, total=tr_dataloader.__len__()):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tr_xs = tr_batch['tokens_ids'].to(device)\n",
    "        tr_ys = tr_batch['tags_ids'].to(device)\n",
    "        \n",
    "        # Calculate loss\n",
    "        tr_emission_scores = model(x=tr_xs).to(device) # size: [batch=128, seq_len=100, 10]\n",
    "        tr_loss = model.loss_fn(emission_scores=tr_emission_scores, tags=tr_ys, mask=(tr_ys > 0).bool())\n",
    "        tr_losses.append(tr_loss.item())\n",
    "        \n",
    "        # Calculate total loss\n",
    "        total_loss = tr_loss + model.regularization_loss_fn(lam=config.REG_LAMBDA, alpha=config.REG_ALPHA)\n",
    "        \n",
    "        # Backward pass: compute gradient of the loss w.r.t. all learnable parameters\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Clip computed gradients\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=config.MAX_GRAD_NORM)\n",
    "        \n",
    "        # Optimize: update the weights using Adam optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "    # END TRAINING PHASE AND UPDATE LOG\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f\"Epoch: {epoch:02d} NLL:{tr_loss.item()}\")\n",
    "        writer.add_scalar('tr/'+'loss', np.mean(tr_losses), global_step=epoch)\n",
    "        writer.add_scalar('tr/'+'total_grad_norm', grad_norm, global_step=epoch)\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram('tr/'+name, param.data, global_step=epoch)\n",
    "        print(\"tr loss\", np.mean(tr_losses))\n",
    "        \n",
    "    # VALIDATION PHASE\n",
    "    \n",
    "    va_losses = []\n",
    "    \n",
    "    batch_preds = []\n",
    "    batch_trues = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for va_batch in tqdm(va_dataloader, total=va_dataloader.__len__()):\n",
    "            va_xs = va_batch['tokens_ids'].to(device) # size: [batch=128, seq_len=100]\n",
    "            va_ys = va_batch['tags_ids'].to(device) # size: [batch=128, seq_len=100]\n",
    "\n",
    "            # Forward pass: compute predicted output by passing input to the model\n",
    "            va_emission_scores = model(x=va_xs).to(device) # size: [batch=128, seq_len=100]\n",
    "            va_preds = torch.tensor(model.decode(va_emission_scores)).to(device)\n",
    "            va_loss = model.loss_fn(emission_scores=va_emission_scores, tags=va_ys, mask=(va_ys > 0).bool())\n",
    "            va_losses.append(va_loss.item())\n",
    "            \n",
    "            mask = (va_ys > 0).bool()\n",
    "\n",
    "            for row_id, true in enumerate(va_ys):\n",
    "                # do not count padding\n",
    "                true_tags = true[mask[row_id]]\n",
    "                # idx2tag\n",
    "                true_tags = [IDX2TAG[idx] for idx in true_tags.tolist()]\n",
    "                # convert to the format expected by seqeval\n",
    "                true_tags = io2bio(true_tags)\n",
    "                batch_trues.append(true_tags)\n",
    "\n",
    "            for row_id, pred in enumerate(va_preds):\n",
    "                # do not count padding\n",
    "                pred_tags = pred[mask[row_id]]\n",
    "                # idx2tag\n",
    "                pred_tags = [IDX2TAG[idx] for idx in pred_tags.tolist()]\n",
    "                # convert to the format expected by seqeval\n",
    "                pred_tags = io2bio(pred_tags)\n",
    "                batch_preds.append(pred_tags)\n",
    "            \n",
    "        for i in range(5):\n",
    "            print('pred:', batch_preds[i])\n",
    "            print('true:', batch_trues[i])\n",
    "            print()\n",
    "\n",
    "        print(\"va loss\", np.mean(va_losses))\n",
    "        writer.add_scalar('va/'+'loss', np.mean(va_losses), global_step=epoch)\n",
    "\n",
    "        report = classification_report(y_true=batch_trues, y_pred=batch_preds, zero_division=0)\n",
    "        print(report)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"weights/model_epoch_{epoch:02d}.pt\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=artefacts/fine_grained/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
