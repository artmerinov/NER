{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "from utils import set_random_seed, Config, IO2BIO\n",
    "from dataset import IO2df, MyDataset\n",
    "from model import BiLSTM_CRF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_random_seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.tensorboard as tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('config.yaml')\n",
    "\n",
    "tr_titles = IO2df(config.TR_PATH)\n",
    "va_titles = IO2df(config.VA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag -> tagID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG2IDX = {\n",
    "    \"O\": 1,\n",
    "    \"art\": 2,\n",
    "    \"building\": 3,\n",
    "    \"event\": 4,\n",
    "    \"location\": 5,\n",
    "    \"organization\": 6,\n",
    "    \"other\": 7,\n",
    "    \"person\": 8,\n",
    "    \"product\": 9,\n",
    "}\n",
    "\n",
    "IDX2TAG = {i: t for t, i in TAG2IDX.items()}\n",
    "IDX2TAG[0] = '[PAD]' # in case if model predicts padding id (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles['tags_ids'] = tr_titles['tags'].transform(lambda x: [TAG2IDX[tag] for tag in x])\n",
    "va_titles['tags_ids'] = va_titles['tags'].transform(lambda x: [TAG2IDX[tag] for tag in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token -> tokenID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_token_cntr(filepath):\n",
    "\n",
    "    token_cntr = Counter()\n",
    "    num_lines = sum(1 for _ in open(filepath, encoding=\"utf-8\"))\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, total=num_lines):\n",
    "            line = line.strip().split()\n",
    "            if line:\n",
    "                token, fine_tag = line\n",
    "                token_cntr[token] += 1\n",
    "    \n",
    "    return token_cntr\n",
    "\n",
    "tr_token_cntr = calc_token_cntr(filepath=config.TR_PATH)\n",
    "va_token_cntr = calc_token_cntr(filepath=config.VA_PATH)\n",
    "token_cntr = tr_token_cntr + va_token_cntr\n",
    "\n",
    "MC = 50_000\n",
    "top_tokens = [token for token, _ in token_cntr.most_common(MC)]\n",
    "TOKEN2IDX = {token: i + 1 for i, token in enumerate(top_tokens)}\n",
    "for token in set(token_cntr) - set(top_tokens):\n",
    "    TOKEN2IDX[token] = len(top_tokens) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles['tokens_ids'] = tr_titles['tokens'].transform(lambda x: [TOKEN2IDX[token] for token in x])\n",
    "va_titles['tokens_ids'] = va_titles['tokens'].transform(lambda x: [TOKEN2IDX[token] for token in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(ids, max_len = 100):\n",
    "    if len(ids) >= max_len:\n",
    "        return ids[:max_len]\n",
    "    ids.extend([0]*(max_len-len(ids)))\n",
    "    return ids\n",
    "\n",
    "tr_titles['tokens_ids'] = tr_titles['tokens_ids'].transform(padding, max_len=100)\n",
    "tr_titles['tags_ids'] = tr_titles['tags_ids'].transform(padding, max_len=100)\n",
    "\n",
    "va_titles['tokens_ids'] = va_titles['tokens_ids'].transform(padding, max_len=100)\n",
    "va_titles['tags_ids'] = va_titles['tags_ids'].transform(padding, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = MyDataset(tr_titles)\n",
    "va_dataset = MyDataset(va_titles)\n",
    "\n",
    "tr_dataloader = DataLoader(dataset=tr_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "va_dataloader = DataLoader(dataset=va_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "tr_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = BiLSTM_CRF(\n",
    "    embed_size     = 100,\n",
    "    hidden_size    = 256, \n",
    "    dropout        = 0.5,\n",
    "    token_voc_size = len(TOKEN2IDX) + 1, \n",
    "    tag_voc_size   = len(TAG2IDX) + 1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = 'weights'\n",
    "if not os.path.exists(weights_folder):\n",
    "    os.makedirs(weights_folder)\n",
    "    \n",
    "runs_folder = '.runs'\n",
    "if not os.path.exists(runs_folder):\n",
    "    os.makedirs(runs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Make tensorboard writer\n",
    "writer = tensorboard.SummaryWriter(log_dir='./runs')\n",
    "\n",
    "global_tr_losses = []\n",
    "global_va_losses = []\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    # TRAINING PHASE\n",
    "    \n",
    "    tr_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for tr_batch in tqdm(tr_dataloader, total=tr_dataloader.__len__()):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tr_xs = tr_batch['tokens_ids'].to(device)\n",
    "        tr_ys = tr_batch['tags_ids'].to(device)\n",
    "        \n",
    "        # Calculate loss\n",
    "        tr_emission_scores = model(tr_xs).to(device) # size: [batch=128, seq_len=100, 10]\n",
    "        tr_loss = model.loss_fn(emission_scores=tr_emission_scores, tags=tr_ys, mask=(tr_ys > 0).bool())\n",
    "        tr_losses.append(tr_loss.item())\n",
    "        \n",
    "        # Calculate total loss\n",
    "        total_loss = tr_loss + model.regularization_loss_fn(lam=1e-3, alpha=0.5)\n",
    "        \n",
    "        # Backward pass: compute gradient of the loss w.r.t. all learnable parameters\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Clip computed gradients\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1e2)\n",
    "        \n",
    "        # Optimize: update the weights using Adam\n",
    "        optimizer.step()\n",
    "        \n",
    "    # END TRAINING PHASE AND UPDATE LOG\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f\"Epoch: {epoch:02d} NLL:{tr_loss.item()}\")\n",
    "        writer.add_scalar('tr/'+'loss', np.mean(tr_losses), global_step=epoch)\n",
    "        writer.add_scalar('tr/'+'total_grad_norm', grad_norm, global_step=epoch)\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram('tr/'+name, param.data, global_step=epoch)\n",
    "        print(\"tr loss\", np.mean(tr_losses))\n",
    "        \n",
    "    # VALIDATION PHASE\n",
    "    \n",
    "    va_losses = []\n",
    "    \n",
    "    batch_preds = []\n",
    "    batch_trues = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for va_batch in tqdm(va_dataloader, total=va_dataloader.__len__()):\n",
    "            va_xs = va_batch['tokens_ids'].to(device) # size: [batch=128, seq_len=100]\n",
    "            va_ys = va_batch['tags_ids'].to(device) # size: [batch=128, seq_len=100]\n",
    "\n",
    "            # Forward pass: compute predicted output by passing input to the model\n",
    "            va_emission_scores = model(va_xs).to(device) # size: [batch=128, seq_len=100]\n",
    "            va_preds = model.decode(va_emission_scores)\n",
    "            va_loss = model.loss_fn(emission_scores=va_emission_scores, tags=va_ys, mask=(va_ys > 0).bool())\n",
    "            va_losses.append(va_loss.item())\n",
    "            \n",
    "            # find max length without PADDING (padding value is 0) for each row in a batch\n",
    "            title_length = torch.sum(va_ys > 0, dim=1) # size: [batch=128]\n",
    "\n",
    "            for row_id, true in enumerate(va_ys.tolist()):\n",
    "                # do not count padding\n",
    "                true_tags = [IDX2TAG[idx] for idx in true[:title_length[row_id]]]\n",
    "                # convert to the format expected by seqeval\n",
    "                true_tags = IO2BIO(true_tags)\n",
    "                batch_trues.append(true_tags)\n",
    "\n",
    "            for row_id, pred in enumerate(va_preds):\n",
    "                # do not count padding\n",
    "                pred_tags = [IDX2TAG[idx] for idx in pred[:title_length[row_id]]]\n",
    "                # convert to the format expected by seqeval\n",
    "                pred_tags = IO2BIO(pred_tags)\n",
    "                batch_preds.append(pred_tags)\n",
    "            \n",
    "\n",
    "        for i in range(5):\n",
    "            print('pred:', batch_preds[i])\n",
    "            print('true:', batch_trues[i])\n",
    "            print()\n",
    "\n",
    "        print(\"va loss\", np.mean(va_losses))\n",
    "        writer.add_scalar('va/'+'loss', np.mean(va_losses), global_step=epoch)\n",
    "\n",
    "        report = classification_report(y_true=batch_trues, y_pred=batch_preds, zero_division=0)\n",
    "        print(report)\n",
    "\n",
    "\n",
    "    global_tr_losses.append(np.mean(tr_losses))\n",
    "    global_va_losses.append(np.mean(va_losses))\n",
    "    torch.save(model.state_dict(), f\"weights/model_epoch_{epoch:02d}.pt\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch: 04 NLL:171.06204223632812\n",
    "# tr loss 299.8672805786133\n",
    "# 100%|██████████| 148/148 [00:42<00:00,  3.49it/s]\n",
    "# pred: ['B-other', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-other', 'O', 'O', 'O']\n",
    "# true: ['O', 'O', 'O', 'O', 'B-other', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-other', 'O', 'O', 'O']\n",
    "\n",
    "# pred: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-location', 'I-location', 'O']\n",
    "# true: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-location', 'I-location', 'O']\n",
    "\n",
    "# pred: ['O', 'O', 'B-person', 'O', 'O', 'B-person', 'O', 'O', 'O', 'O', 'O', 'B-person', 'I-person', 'O', 'O', 'O', 'O', 'O']\n",
    "# true: ['O', 'O', 'B-art', 'I-art', 'O', 'B-person', 'O', 'O', 'O', 'O', 'O', 'B-person', 'I-person', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "# pred: ['O', 'O', 'B-person', 'I-person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-art', 'I-art', 'I-art', 'O', 'O', 'O', 'O', 'O', 'O', 'B-person', 'I-person', 'O', 'O', 'B-art', 'I-art', 'O', 'O', 'O', 'B-art', 'I-art', 'O', 'O', 'O', 'O', 'B-person', 'I-person', 'I-person', 'O', 'O', 'O', 'B-person', 'I-person', 'O', 'O', 'B-person', 'O', 'B-person', 'I-person', 'I-person', 'O', 'B-person', 'O', 'O', 'O', 'O', 'O', 'O', 'B-person', 'I-person', 'I-person', 'O', 'O', 'B-art', 'I-art', 'I-art', 'I-art', 'I-art', 'I-art', 'I-art', 'I-art', 'I-art', 'I-art', 'O', 'B-art', 'I-art', 'I-art', 'I-art', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "# true: ['O', 'O', 'B-person', 'I-person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-art', 'I-art', 'I-art', 'O', 'O', 'O', 'O', 'O', 'O', 'B-person', 'I-person', 'O', 'O', 'B-art', 'I-art', 'O', 'O', 'O', 'B-art', 'I-art', 'O', 'O', 'O', 'O', 'O', 'B-person', 'I-person', 'O', 'O', 'O', 'B-person', 'I-person', 'O', 'O', 'B-art', 'O', 'B-art', 'I-art', 'I-art', 'O', 'B-art', 'O', 'O', 'O', 'O', 'O', 'O', 'B-person', 'I-person', 'I-person', 'O', 'O', 'B-art', 'I-art', 'I-art', 'I-art', 'I-art', 'O', 'B-art', 'I-art', 'I-art', 'I-art', 'O', 'B-art', 'I-art', 'I-art', 'I-art', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "# pred: ['O', 'B-other', 'I-other', 'O', 'B-other', 'I-other', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-other', 'O', 'O', 'O', 'O']\n",
    "# true: ['O', 'B-other', 'O', 'O', 'B-other', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-other', 'O', 'O', 'O', 'O']\n",
    "\n",
    "# va loss 487.23016816216546\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#          art       0.69      0.67      0.68      2063\n",
    "#     building       0.64      0.59      0.61      2484\n",
    "#        event       0.62      0.57      0.59      2034\n",
    "#     location       0.78      0.79      0.78     13649\n",
    "# organization       0.64      0.63      0.64      9585\n",
    "#        other       0.63      0.54      0.58      4958\n",
    "#       person       0.80      0.85      0.82     10954\n",
    "#      product       0.65      0.44      0.52      2955\n",
    "\n",
    "#    micro avg       0.72      0.70      0.71     48682\n",
    "#    macro avg       0.68      0.63      0.65     48682\n",
    "# weighted avg       0.72      0.70      0.71     48682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
