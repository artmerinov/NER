{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "from tokenizer import make_word_vocab, make_char_vocab\n",
    "from prepare_data import read_raw_data, preprocess_raw_data\n",
    "from dataset import NERDataset\n",
    "from model import CNN_BiRNN_CRF\n",
    "from utils import set_random_seed, Config, load_json, make_padding, io2bio\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_random_seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TR_PATH': 'data/supervised/train.txt',\n",
       " 'VA_PATH': 'data/supervised/dev.txt',\n",
       " 'TE_PATH': 'data/supervised/test.txt',\n",
       " 'MAX_SEQ_LEN': 64,\n",
       " 'MAX_WORD_LEN': 16,\n",
       " 'WORD_SUPPORT': 10,\n",
       " 'CHAR_SUPPORT': 100,\n",
       " 'BATCH_SIZE': 128,\n",
       " 'NUM_EPOCHS': 10,\n",
       " 'LR': 0.001,\n",
       " 'REG_LAMBDA': 0.0001,\n",
       " 'MAX_GRAD_NORM': 100,\n",
       " 'WORD_EMBED_SIZE': 128,\n",
       " 'CHAR_EMBED_SIZE': 128,\n",
       " 'KERNEL_SIZE': 3,\n",
       " 'RNN_CELL': 'LSTM',\n",
       " 'RNN_HIDDEN_SIZE': 128,\n",
       " 'NUM_LAYERS': 1,\n",
       " 'DROPOUT': 0.5,\n",
       " 'SKIP_CONNECTION': False}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config('config.yaml')\n",
    "config.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_titles = read_raw_data(filepath=config.TR_PATH)\n",
    "va_titles = read_raw_data(filepath=config.VA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                 37661\n",
       "words                  [The, Amsterdam, Science, Park, railway, stati...\n",
       "tags_fine_grained      [O, location-park, location-park, location-par...\n",
       "tags_coarse_grained    [O, location, location, location, O, O, O, O, ...\n",
       "Name: 37661, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_titles.loc[37661]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag -> tagID\n",
    "TAG2IDX = load_json('ner_tags/ner_fine_grained.json')\n",
    "IDX2TAG = {i: t for t, i in TAG2IDX.items()}\n",
    "\n",
    "# word -> wordID\n",
    "WORD2IDX = make_word_vocab(filepath=config.TR_PATH, support=config.WORD_SUPPORT, save_path='tokenizers/word2idx.json')\n",
    "IDX2WORD = {i: w for w, i in WORD2IDX.items()}\n",
    "\n",
    "# char -> charID\n",
    "CHAR2IDX = make_char_vocab(filepath=config.TR_PATH, support=config.CHAR_SUPPORT, save_path='tokenizers/char2idx.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply TAG2IDX\n",
    "tr_titles['tag_ids'] = tr_titles['tags_fine_grained'].transform(lambda x: [TAG2IDX[tag] for tag in x])\n",
    "va_titles['tag_ids'] = va_titles['tags_fine_grained'].transform(lambda x: [TAG2IDX[tag] for tag in x])\n",
    "\n",
    "# apply WORD2IDX\n",
    "tr_titles['word_ids'] = tr_titles['words'].transform(lambda x: [WORD2IDX[w] if w in WORD2IDX else WORD2IDX['UKN'] for w in x])\n",
    "va_titles['word_ids'] = va_titles['words'].transform(lambda x: [WORD2IDX[w] if w in WORD2IDX else WORD2IDX['UKN'] for w in x])\n",
    "\n",
    "# apply padding\n",
    "tr_titles['word_ids'] = tr_titles['word_ids'].transform(make_padding, max_len=config.MAX_SEQ_LEN)\n",
    "tr_titles['tag_ids'] = tr_titles['tag_ids'].transform(make_padding, max_len=config.MAX_SEQ_LEN)\n",
    "\n",
    "va_titles['word_ids'] = va_titles['word_ids'].transform(make_padding, max_len=config.MAX_SEQ_LEN)\n",
    "va_titles['tag_ids'] = va_titles['tag_ids'].transform(make_padding, max_len=config.MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                 37661\n",
       "words                  [The, Amsterdam, Science, Park, railway, stati...\n",
       "tags_fine_grained      [O, location-park, location-park, location-par...\n",
       "tags_coarse_grained    [O, location, location, location, O, O, O, O, ...\n",
       "tag_ids                [1, 27, 27, 27, 1, 1, 1, 1, 1, 1, 26, 26, 26, ...\n",
       "word_ids               [12, 2933, 729, 175, 631, 186, 2349, 2, 160, 8...\n",
       "Name: 37661, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_titles.loc[37661]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131767/131767 [00:29<00:00, 4469.31it/s]\n",
      "100%|██████████| 18824/18824 [00:04<00:00, 4005.66it/s]\n"
     ]
    }
   ],
   "source": [
    "tr_dataset = NERDataset(data=tr_titles, idx2word=IDX2WORD, char2idx=CHAR2IDX, max_word_len=config.MAX_WORD_LEN)\n",
    "va_dataset = NERDataset(data=va_titles, idx2word=IDX2WORD, char2idx=CHAR2IDX, max_word_len=config.MAX_WORD_LEN)\n",
    "\n",
    "tr_dataloader = DataLoader(dataset=tr_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "va_dataloader = DataLoader(dataset=va_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_ids': tensor([   35,  1601, 15202,    22,   659,     3,     1,     1,     6,  5586,\n",
       "          8024,     4,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]),\n",
       " 'char_ids': tensor([[37,  4,  0,  ...,  0,  0,  0],\n",
       "         [ 9,  4,  3,  ...,  0,  0,  0],\n",
       "         [40,  6, 13,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [38, 29, 43,  ...,  0,  0,  0],\n",
       "         [38, 29, 43,  ...,  0,  0,  0],\n",
       "         [38, 29, 43,  ...,  0,  0,  0]]),\n",
       " 'tag_ids': tensor([ 1,  1, 52,  1,  1,  1, 51, 51,  1, 51, 51,  1,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in tr_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 64]), torch.Size([128, 64, 16]), torch.Size([128, 64]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['word_ids'].size(), data['char_ids'].size(), data['tag_ids'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embedding.weight                    initialized w with Xavier            parameters #: 2530304\n",
      "char_embedding.char_embedding.weight     initialized w with Xavier            parameters #: 15488\n",
      "char_embedding.char_conv.weight          initialized w with Xavier            parameters #: 49152\n",
      "char_embedding.char_conv.bias            initialized b with zero              parameters #: 128\n",
      "rnn.weight_ih_l0                         initialized w with Xavier            parameters #: 65536\n",
      "rnn.weight_hh_l0                         initialized w with Xavier            parameters #: 16384\n",
      "rnn.weight_ih_l0_reverse                 initialized w with Xavier            parameters #: 65536\n",
      "rnn.weight_hh_l0_reverse                 initialized w with Xavier            parameters #: 16384\n",
      "fc.weight                                initialized w with Xavier            parameters #: 8704\n",
      "fc.bias                                  initialized b with zero              parameters #: 68\n",
      "crf.start_transitions                    initialized b with zero              parameters #: 68\n",
      "crf.end_transitions                      initialized b with zero              parameters #: 68\n",
      "crf.transitions                          initialized w with Xavier            parameters #: 4624\n",
      "CNN_BiRNN_CRF(\n",
      "  (word_embedding): Embedding(19768, 128, padding_idx=0)\n",
      "  (char_embedding): CharCNN(\n",
      "    (char_embedding): Embedding(121, 128, padding_idx=0)\n",
      "    (char_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (rnn): LSTM(256, 64, bias=False, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=68, bias=True)\n",
      "  (crf): CRF(num_tags=68)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = CNN_BiRNN_CRF(\n",
    "    word_embed_size  = config.WORD_EMBED_SIZE,\n",
    "    char_embed_size  = config.CHAR_EMBED_SIZE,\n",
    "    kernel_size      = config.KERNEL_SIZE,\n",
    "    rnn_cell         = config.RNN_CELL,\n",
    "    rnn_hidden_size  = config.RNN_HIDDEN_SIZE,\n",
    "    dropout          = config.DROPOUT,\n",
    "    num_layers       = config.NUM_LAYERS,\n",
    "    skip_connection  = config.SKIP_CONNECTION,\n",
    "    word_voc_size    = len(WORD2IDX),\n",
    "    char_voc_size    = len(CHAR2IDX),\n",
    "    tag_voc_size     = len(TAG2IDX),\n",
    ").to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = 'weights'\n",
    "if not os.path.exists(weights_folder):\n",
    "    os.makedirs(weights_folder)\n",
    "    \n",
    "runs_folder = 'runs'\n",
    "if not os.path.exists(runs_folder):\n",
    "    os.makedirs(runs_folder)\n",
    "\n",
    "experiment_folder = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "if not os.path.exists(f'{runs_folder}/{experiment_folder}'):\n",
    "    os.makedirs(f'{runs_folder}/{experiment_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.LR)\n",
    "\n",
    "# Make tensorboard writer\n",
    "writer = tensorboard.SummaryWriter(log_dir=f'{runs_folder}/{experiment_folder}')\n",
    "writer.add_text('Parameters', \" \".join([f\"{k}={v}\" for k,v in config.__dict__.items()]))\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "\n",
    "    # TRAINING PHASE\n",
    "    \n",
    "    tr_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for batch_id, tr_batch in enumerate(tr_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tr_xs = tr_batch['word_ids'].to(device) # [batch_size, max_seq_len]\n",
    "        tr_cs = tr_batch['char_ids'].to(device) # [batch_size, max_seq_len, max_word_len]\n",
    "        tr_ys = tr_batch['tag_ids'].to(device) # [batch_size, max_seq_len]\n",
    "        tr_mask = (tr_ys > 0).bool()\n",
    "        \n",
    "        tr_emission_scores = model(word_ids=tr_xs, char_ids=tr_cs).to(device) # [batch_size, max_seq_len, max_word_len]\n",
    "        tr_loss = model.loss_fn(emission_scores=tr_emission_scores, tags=tr_ys, mask=tr_mask)\n",
    "        reg_loss = model.regularization_loss_fn(lam=config.REG_LAMBDA)\n",
    "        total_loss = tr_loss + reg_loss\n",
    "        tr_losses.append(tr_loss.item())\n",
    "        \n",
    "        if batch_id % 100 == 0:\n",
    "            print(\n",
    "                f\"epoch={epoch:02d}\",\n",
    "                f\"batch_id={batch_id:04d}/{len(tr_dataloader)}\",\n",
    "                f\"crf_loss={tr_loss:.2f}\",\n",
    "                f\"reg_loss={reg_loss:.2f}\",\n",
    "                f\"total_loss={total_loss:.2f}\"\n",
    "            )\n",
    "\n",
    "        # Backward pass: compute gradient of the loss w.r.t. all learnable parameters\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Clip computed gradients\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=config.MAX_GRAD_NORM)\n",
    "        \n",
    "        # Optimize: update the weights using Adam optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "    # VALIDATION PHASE\n",
    "    \n",
    "    va_losses = []\n",
    "    \n",
    "    batch_preds = []\n",
    "    batch_trues = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for va_batch in tqdm(va_dataloader, total=len(va_dataloader)):\n",
    "            \n",
    "            va_xs = va_batch['word_ids'].to(device) # [batch_size, max_seq_len]\n",
    "            va_cs = va_batch['char_ids'].to(device) # [batch_size, max_seq_len, max_word_len]\n",
    "            va_ys = va_batch['tag_ids'].to(device) # [batch_size, max_seq_len]\n",
    "            va_mask = (va_ys > 0).bool()\n",
    "\n",
    "            # Forward pass: compute predicted output by passing input to the model\n",
    "            va_emission_scores = model(word_ids=va_xs, char_ids=va_cs).to(device) # [batch_size, max_seq_len]\n",
    "            va_preds = torch.tensor(model.decode(va_emission_scores)).to(device)\n",
    "            va_loss = model.loss_fn(emission_scores=va_emission_scores, tags=va_ys, mask=va_mask)\n",
    "            va_losses.append(va_loss.item())\n",
    "\n",
    "            for row_id, true in enumerate(va_ys):\n",
    "                # do not count padding\n",
    "                true_tags = true[va_mask[row_id]]\n",
    "                # idx2tag\n",
    "                true_tags = [IDX2TAG[idx] for idx in true_tags.tolist()]\n",
    "                # convert to the format expected by seqeval\n",
    "                true_tags = io2bio(true_tags)\n",
    "                batch_trues.append(true_tags)\n",
    "\n",
    "            for row_id, pred in enumerate(va_preds):\n",
    "                # do not count padding\n",
    "                pred_tags = pred[va_mask[row_id]]\n",
    "                # idx2tag\n",
    "                pred_tags = [IDX2TAG[idx] for idx in pred_tags.tolist()]\n",
    "                # convert to the format expected by seqeval\n",
    "                pred_tags = io2bio(pred_tags)\n",
    "                batch_preds.append(pred_tags)\n",
    "        \n",
    "#         for i in range(5):\n",
    "#             print('pred:', batch_preds[i])\n",
    "#             print('true:', batch_trues[i])\n",
    "#             print()\n",
    "\n",
    "    writer.add_scalar('tr/'+'loss', np.mean(tr_losses), global_step=epoch)\n",
    "    writer.add_scalar('va/'+'loss', np.mean(va_losses), global_step=epoch)\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram('tr/' + name + '_weight', param.data, global_step=epoch)\n",
    "        writer.add_histogram('tr/' + name + '_grad', param.grad, global_step=epoch)\n",
    "\n",
    "    print(\n",
    "        f\"tr_avg_loss={np.mean(tr_losses)}\", \n",
    "        f\"va_avg_loss={np.mean(va_losses)}\"\n",
    "    )\n",
    "    \n",
    "    report = classification_report(y_true=batch_trues, y_pred=batch_preds, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"weights/model_epoch_{epoch:02d}.pt\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
